{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"t9ThLOk-1uem"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","import tensorflow as tf\n","from tensorflow.keras.datasets import cifar100\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n","from tensorflow.keras.optimizers import SGD\n","from tensorflow.keras.losses import CategoricalCrossentropy\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"]},{"cell_type":"markdown","source":["Imported all the required libaries and CIFAR100 datset"],"metadata":{"id":"LB_7rMzQegpr"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"vGUWn6_1_ZIO"},"outputs":[],"source":["\n","batch_size = 64\n","learning_rate = 3e-3\n","num_epochs = 50\n","\n","# Data preprocessing and augmentation\n","data_gen = ImageDataGenerator(\n","    rescale=1./255,\n","    horizontal_flip=True,\n","    featurewise_center=True,\n","    featurewise_std_normalization=True)"]},{"cell_type":"markdown","source":["I have set the batch size to 64. I tried increasing and decreasing it and found this to be most optimum. Number of epochs that is iterations are currently set to 50 and can be further increased to increase the accuracy. Learning rate(used in optimization) has been set to 3e-3\n","\n","Data preprocessing is also done in this step.\n","I divided it by 255 as rgb has value from 0-255 for each color"],"metadata":{"id":"8b-zK0dJfP5C"}},{"cell_type":"code","source":["\n","# Load the CIFAR-100 dataset\n","(x_train, y_train), (x_test, y_test) = cifar100.load_data()"],"metadata":{"id":"rH9kKQhhb4Xl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Loaded CIFAR100 dataset and splitted it for training and testing"],"metadata":{"id":"nqBL9jAcf4M1"}},{"cell_type":"code","source":["\n","# Convert labels to categorical format\n","y_train = tf.keras.utils.to_categorical(y_train, num_classes=100)\n","y_test = tf.keras.utils.to_categorical(y_test, num_classes=100)\n"],"metadata":{"id":"_-9uzJApb7Kt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Converted training and testing labels to one-hot encoded categorical format with 100 classes."],"metadata":{"id":"MLIpDuXNgNoi"}},{"cell_type":"code","source":["data_gen.fit(x_train)\n","# Create data generators\n","train_generator = data_gen.flow(x_train, y_train, batch_size=batch_size)\n","test_generator = data_gen.flow(x_test, y_test, batch_size=batch_size, shuffle=False)\n"],"metadata":{"id":"W0ibLvjCb9lr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Made batches with size equal to 64(set before) of already processed data"],"metadata":{"id":"oC_HDjwEglCN"}},{"cell_type":"code","source":["\n","# Build the model\n","model = Sequential([\n","    Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)),\n","    MaxPooling2D((2, 2)),\n","    Conv2D(64, (3, 3), activation='relu', padding='same'),\n","    MaxPooling2D((2, 2)),\n","    Flatten(),\n","    Dense(1024, activation='relu'),\n","    Dropout(0.5),\n","    Dense(512, activation='relu'),\n","    Dropout(0.2),\n","    Dense(256, activation='relu'),\n","    Dropout(0.2),\n","    Dense(128, activation='relu'),\n","    Dropout(0.2),\n","    Dense(100, activation='softmax')\n","])\n"],"metadata":{"id":"KqSYt8w2cAxv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" I defined a convolutional neural network (CNN) from scratch using the Sequential API.I made a model which includes alternating convolutional layers with ReLU activation and max-pooling layers for feature extraction. Then, the extracted features are flattened and passed through fully connected layers with ReLU activation and dropout regularization to prevent overfitting. Finally, the output layer with softmax activation predicts the probabilities of the input image belonging to each of the 100 classes.\"\n","\n"],"metadata":{"id":"Fw-8wupahHC5"}},{"cell_type":"code","source":["\n","# Compile the model\n","optimizer = SGD(learning_rate=learning_rate, momentum=0.9, weight_decay=5e-4)\n","model.compile(optimizer=optimizer, loss=CategoricalCrossentropy(), metrics=['accuracy'])\n"],"metadata":{"id":"QTBBK18mcDuu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In this part I compiled up my model using SGD optimizer with learning rate= 3e-3\n","\n","I tried using Adam optimizer as well but found this to be more optimum.\n","\n"," For loss calculation, I've used categorical cross-entropy, suitable for multi-class classification tasks.I'm also monitoring the accuracy metric to evaluate the model's performance.\n","\n","\n","\n","\n"],"metadata":{"id":"IzFpIFB2h2IR"}},{"cell_type":"code","source":["\n","# Train the model\n","history = model.fit(train_generator, epochs=num_epochs, validation_data=test_generator)\n"],"metadata":{"id":"CCxNhhD-cGcr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This is most important part which is training. I am training it using fit function. The model is trained on already processed train_data for specific number of epochs( currently I have set it to 50).\n","\n","Increasing the number of epochs can increase the accuracy.\n","\n","Additionally, I've  provided the test_generator for validation during training to monitor the model's performance on unseen data."],"metadata":{"id":"eKXUbRglivff"}},{"cell_type":"code","source":["\n","# Plot the training and validation loss\n","plt.figure(figsize=(8, 6))\n","plt.plot(history.history['loss'], label='Training Loss')\n","plt.plot(history.history['val_loss'], label='Validation Loss')\n","plt.title('Training and Validation Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()\n","\n","\n","# Plot the training and validation loss\n","plt.figure(figsize=(8, 6))\n","plt.plot(history.history['accuracy'], label='Training accuracy')\n","plt.plot(history.history['val_accuracy'], label='Validation accuracy')\n","plt.title('Training and Validation accuracy')\n","plt.xlabel('Epoch')\n","plt.ylabel('accuracy')\n","plt.legend()\n","plt.show()\n"],"metadata":{"id":"YyOPBQhDcJRR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In this part I plotted loss vs number of epochs and accuracy vs number of epochs to view the progress,\n","\n","I plotted it for validation data as well"],"metadata":{"id":"F0FJL7kTjllP"}},{"cell_type":"code","source":["\n","# Evaluate the model on the test set\n","test_loss, test_acc = model.evaluate(test_generator)\n","print(f'Test loss: {test_loss:.4f}, Test accuracy: {test_acc:.4f}')\n"],"metadata":{"id":"E2hcOxV5cUlK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This is the final testing moment of model where I test my model on preprocessed test_data and calculate loss and accuracy. The accuracy comes out to be 45%\n","\n","The accuracy is not really great but can be further improved by increasing number of epochs or using transfer learning on pre trained models.\n","."],"metadata":{"id":"zyuaXozNkLO3"}},{"cell_type":"code","source":["\n","# Function to display predictions\n","def display_images(model, test_generator, num_images=20):\n","    test_images, test_labels = next(test_generator)\n","    predictions = model.predict(test_images)\n","    class_names = ['apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', 'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', 'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', 'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', 'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion', 'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse', 'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear', 'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine', 'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', 'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table', 'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout', 'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm']\n","\n","    num_cols = 2  # Number of columns in the subplot grid\n","    num_rows = num_images // num_cols  # Number of rows in the subplot grid\n","\n","    plt.figure(figsize=(7, 5))  # Increase the figure size\n","    for i in range(num_images):\n","        plt.subplot(num_rows, num_cols, i + 1)\n","        plt.imshow(test_images[i])\n","        predicted_class = np.argmax(predictions[i])\n","        true_class = np.argmax(test_labels[i])\n","        class_name = class_names[predicted_class]\n","        plt.title(f'Predicted: {class_name}, Actual: {class_names[true_class]}', fontsize=5)  # Increase the font size\n","        plt.axis('off')\n","    plt.tight_layout()\n","    plt.show()\n","\n","display_images(model, test_generator)\n"],"metadata":{"id":"nEwkeY55cVRa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This is just the testing part of my model where I take some random images and run my model on them. Then I print the actual and predicted classification to see how well my code works"],"metadata":{"id":"PNdNRzBQlFxE"}},{"cell_type":"markdown","source":["## **Acknowledgements**\n","\n","1. ***https://github.com/shuoros/cifar100-resnet50-pytorch***\n","\n","    I refered to this code while working on this project. This code is made using\n","    pytorch. I tried to understand this code and tried to implement same using keras and tensorflow.\n","\n","2. ***Open AI ChatGpt***\n","\n","    I also used ChatGpt for helping me with some of keras and tensorflow libraries.\n","\n"],"metadata":{"id":"vpF-5_YRB_ec"}},{"cell_type":"markdown","source":["# **Areas of Improvement**\n","\n","1. **Using Transfer learning**\n","\n","    I think incorporating any pre trained model like Resnet or VGG16 using transfer learning is the best way to increase the model effecincy. I tried using transfer learning but it increased the time so went along with this.\n","\n","2. **Adding more layers**\n","\n","    Adding more layers can also increase the effeciency of the model. This is the basic model I tried to made\n","\n","3. **Increasing Data size**\n","\n","    Increasing the train data size is also a good way to increase accuracy. This will increase the generalisation of the model. For now I went with CIFAR100 which has around 50000 images\n","\n","4. **Changing Learning Rate**\n","\n","    Learning Rate also plays important role in deciding accuracy rate.\n","\n"],"metadata":{"id":"kyzhfNdEDEe9"}},{"cell_type":"markdown","source":["# **Difficulties Faced**\n","\n","\n","\n","\n","1. I was first working with COCO datset but it was very large and hence faced a lot of difficulty in working with the code. I later shifted to CIFAR100 and found it be most optimum.\n","\n","2. I faced a lot of errors while training the model specially in shape\n","   incompatibility and made a lot of modifications to finally rectify it.\n","\n","3. Choosing an optimizer was also an task. I first chose Adam optimizer but found the accuracy lower than SGD optimizer so finally went ahead with this.\n"],"metadata":{"id":"5cwbKZ2kGp5G"}}],"metadata":{"accelerator":"TPU","colab":{"gpuType":"V28","provenance":[{"file_id":"1mlcsthDHC4_2nwXCm4-ocTzphkHTiVw1","timestamp":1715978736957},{"file_id":"1njtju3IT-3hYQuGcnY3CWwC1JLf0cuic","timestamp":1715977933839},{"file_id":"1o_uhUbPGCEAKyrjfvnlSFnrDlW7Bi-FK","timestamp":1715822592586}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}